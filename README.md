# Методы второго порядка

## Набор данных
Сгенерирован датасет синтетических данных размера ${N=2000}$.

В основу генерации положена полиномиальная функция с добавлением случайного шума ${\epsilon}$

${f(x) = \sum_{k=0}^{K-1}{w_k \cdot x^k} + \epsilon}$,

где ${w=[w_0, w_1, ..., w_{K-1}]}$ - массив весов размера ${K}$.

Итоговый датасет [x, y] сгенерирован для ${x \in [0; 3]}$ при следующих весах:
${w=[-5, 2, -3, 1]}$ ${(K=4)}$.

![dataset](plots/dataset_plot.png)

## Постановка задачи
Необходимо найти веса модели, представленной на рисунке ниже.
Количество весов принято большим на одно ${(K=5)}$, в ожидании получить нулевое значение этого искусственно добавленного коэффициента.
![architecture](plots/architecture.png)

## Метод решения
Поиск коэффициентов осуществлялся путем минимизации функции потерь.

${loss(w) = \sum \left(y_i - \bar{y}_i(w) \right)^2}$

Минимизация методом градиентного спуска.

Ожидаемые значения весов: ${w=[-5, 2, -3, 1, 0]}$.

В целях ускорения сходимости метода исследованы следующие гиперпараметры:

- Шаг обучения (learning rate).
- Размер тренировочной выборки (batch size).
- Постоянная $\beta$ фильтра градиентов (Momentum):

${G_n = \beta G_{n-1} + (1 - \beta)grad_n}$

Градиент вычислялся численно, с помощью

## Выводы
1. 'Learning
